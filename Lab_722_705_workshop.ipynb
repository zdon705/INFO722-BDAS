{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/xgboost/__init__.py:29: FutureWarning: Python 3.5 support is deprecated; XGBoost will require Python 3.6+ in the near future. Consider upgrading to Python 3.6+.\n",
      "  FutureWarning)\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/pyspark/context.py:227: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e169353b2c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/home/ubuntu/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7362\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7364\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4871\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4872\u001b[0;31m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4873\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4874\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mform_blocks\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4904\u001b[0m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FloatBlock'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4906\u001b[0;31m         \u001b[0mfloat_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FloatBlock'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4907\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_multi_blockify\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   4993\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4995\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4997\u001b[0m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_stack_arrays\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   5035\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_shape_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5037\u001b[0;31m     \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5038\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5039\u001b[0m         \u001b[0mstacked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46632)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "import pyspark\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('adult').getOrCreate()\n",
    "data_train = spark.read.csv(r'/home/ubuntu/train.csv', inferSchema = True, header=True)\n",
    "pd.DataFrame(data_train.take(20), columns = data_train.columns)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "data_train =pd.read_csv(r'/home/ubuntu/train.csv')\n",
    "data_train.shape\n",
    "\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "\n",
    "print(data_train.columns)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "data_train.head()\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "data_train.info()\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "data_train.describe()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "numerical_fea = list(data_train.select_dtypes(exclude=['object']).columns)\n",
    "category_fea = list(filter(lambda x: x not in numerical_fea,list(data_train.columns)))\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "numerical_fea = list(data_train.select_dtypes(exclude=['object']).columns)\n",
    "category_fea = list(filter(lambda x: x not in numerical_fea,list(data_train.columns)))\n",
    "#过滤数值型类别特征\n",
    "def get_numerical_serial_fea(data,feas):\n",
    "    numerical_serial_fea = []\n",
    "    numerical_noserial_fea = []\n",
    "    for fea in feas:\n",
    "        temp = data[fea].nunique()\n",
    "        if temp <= 10:\n",
    "            numerical_noserial_fea.append(fea)\n",
    "            continue\n",
    "        numerical_serial_fea.append(fea)\n",
    "    return numerical_serial_fea,numerical_noserial_fea\n",
    "numerical_serial_fea,numerical_noserial_fea = get_numerical_serial_fea(data_train,numerical_fea)\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "f = pd.melt(data_train, value_vars=numerical_serial_fea)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\n",
    "g = g.map(sns.distplot, \"value\")\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "#Ploting Transaction Amount Values Distribution\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('Transaction Values Distribution', fontsize=22)\n",
    "plt.subplot(221)\n",
    "sub_plot_1 = sns.distplot(data_train['loanAmnt'])\n",
    "sub_plot_1.set_title(\"loanAmnt Distribuition\", fontsize=18)\n",
    "sub_plot_1.set_xlabel(\"\")\n",
    "sub_plot_1.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(222)\n",
    "sub_plot_2 = sns.distplot(np.log(data_train['loanAmnt']))\n",
    "sub_plot_2.set_title(\"loanAmnt (Log) Distribuition\", fontsize=18)\n",
    "sub_plot_2.set_xlabel(\"\")\n",
    "sub_plot_2.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(data_train[\"employmentLength\"].value_counts(dropna=False)[:20],\n",
    "            data_train[\"employmentLength\"].value_counts(dropna=False).keys()[:20])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "train_loan_fr = data_train.loc[data_train['isDefault'] == 1]\n",
    "train_loan_nofr = data_train.loc[data_train['isDefault'] == 0]\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\n",
    "train_loan_fr.groupby('grade')['grade'].count().plot(kind='barh', ax=ax1, title='Count of grade fraud')\n",
    "train_loan_nofr.groupby('grade')['grade'].count().plot(kind='barh', ax=ax2, title='Count of grade non-fraud')\n",
    "train_loan_fr.groupby('employmentLength')['employmentLength'].count().plot(kind='barh', ax=ax3, title='Count of employmentLength fraud')\n",
    "train_loan_nofr.groupby('employmentLength')['employmentLength'].count().plot(kind='barh', ax=ax4, title='Count of employmentLength non-fraud')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "#查看缺失值情况\n",
    "data_train.isnull().sum()\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "data_train.info()\n",
    "\n",
    "\n",
    "# In[146]:\n",
    "\n",
    "\n",
    "data_train =pd.read_csv(r'/home/ubuntu/train1.csv')\n",
    "data_train.shape\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "print(data_train.columns)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "#查看缺失值情况\n",
    "data_train.isnull().sum()\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('employmentLength',axis=1)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "numerical_fea = list(data_train.select_dtypes(exclude=['object']).columns)\n",
    "category_fea = list(filter(lambda x: x not in numerical_fea,list(data_train.columns)))\n",
    "\n",
    "def get_numerical_serial_fea(data,feas):\n",
    "    numerical_serial_fea = []\n",
    "    numerical_noserial_fea = []\n",
    "    for fea in feas:\n",
    "        temp = data[fea].nunique()\n",
    "        if temp <= 10:\n",
    "            numerical_noserial_fea.append(fea)\n",
    "            continue\n",
    "        numerical_serial_fea.append(fea)\n",
    "    return numerical_serial_fea,numerical_noserial_fea\n",
    "numerical_serial_fea,numerical_noserial_fea = get_numerical_serial_fea(data_train,numerical_fea)\n",
    "\n",
    "data_train[numerical_fea] = data_train[numerical_fea].fillna(data_train[numerical_fea].median())\n",
    "data_train[category_fea] = data_train[category_fea].fillna(data_train[category_fea].mode())\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "data_train.isnull().sum()\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "data_train['installment']=abs(data_train['installment'])\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "data_train['installment'].head()\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "print(data_train.columns)\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "data_train.head()\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "data_train.isnull().sum()\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "data_train.shape\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "data_train.info()\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "#label-encode:subGrade,postCode,title\n",
    "for col in tqdm(['employmentTitle', 'postCode', 'title','subGrade']):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(data_train[col].astype(str).values) )\n",
    "    data_train[col] = le.transform(list(data_train[col].astype(str).values))\n",
    "print('Label Encoding ')\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "data_numeric = data_train[numerical_fea]\n",
    "correlation = data_numeric.corr()\n",
    "\n",
    "f , ax = plt.subplots(figsize = (7, 7))\n",
    "plt.title('Correlation of Numeric Features with Price',y=1,size=16)\n",
    "sns.heatmap(correlation,square = True,  vmax=0.8)\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "\n",
    "for data in [data_train]:\n",
    "    data.drop(['issueDate','id'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "total = len(data_train)\n",
    "total_amt = data_train.groupby(['isDefault'])['loanAmnt'].sum().sum()\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plot_tr = sns.countplot(x='isDefault',data=data_train)#data_train‘isDefault’\n",
    "plot_tr.set_title(\"Fraud Loan Distribution \\n 0: good user | 1: bad user\", fontsize=14)\n",
    "plot_tr.set_xlabel(\"Is fraud by count\", fontsize=16)\n",
    "plot_tr.set_ylabel('Count', fontsize=16)\n",
    "for p in plot_tr.patches:\n",
    "    height = p.get_height()\n",
    "    plot_tr.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=15) \n",
    "    \n",
    "percent_amt = (data_train.groupby(['isDefault'])['loanAmnt'].sum())\n",
    "percent_amt = percent_amt.reset_index()\n",
    "plt.subplot(122)\n",
    "plot_tr_2 = sns.barplot(x='isDefault', y='loanAmnt',  dodge=True, data=percent_amt)\n",
    "plot_tr_2.set_title(\"Total Amount in loanAmnt  \\n 0: good user | 1: bad user\", fontsize=14)\n",
    "plot_tr_2.set_xlabel(\"Is fraud by percent\", fontsize=16)\n",
    "plot_tr_2.set_ylabel('Total Loan Amount Scalar', fontsize=16)\n",
    "for p in plot_tr_2.patches:\n",
    "    height = p.get_height()\n",
    "    plot_tr_2.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total_amt * 100),\n",
    "            ha=\"center\", fontsize=15)     \n",
    "\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('postCode',axis=1)\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('title',axis=1)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('subGrade',axis=1)\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('grade',axis=1)\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('earliesCreditLine',axis=1)\n",
    "\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "\n",
    "data_train=data_train.drop('policyCode',axis=1)\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "features = [f for f in data_train.columns if f not in ['id','issueDate','isDefault','postCode','grade','title','subGrade'] ]\n",
    "x = data_train[features]\n",
    "y=data_train[\"isDefault\"]\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "data_train.info()\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "\n",
    "x.columns\n",
    "\n",
    "\n",
    "# In[86]:\n",
    "\n",
    "\n",
    "data_train.head()\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=3, train_size=0.7)\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# In[94]:\n",
    "\n",
    "\n",
    "#模型特征输出\n",
    "importance_value = rf.feature_importances_\n",
    "importance_var = list(x_train.columns)\n",
    "importance_var_value = []\n",
    "for i in range(len(importance_var)):\n",
    "    importance_var_value.append([importance_var[i], importance_value[i]])\n",
    "print(importance_var_value)\n",
    "importance_df = pd.DataFrame()\n",
    "importance_df = importance_df.append(importance_var_value)\n",
    "\n",
    "\n",
    "# In[178]:\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "print (classification_report(y_test,y_pred,target_names=['class_0','class_1'],digits=4)) \n",
    "\n",
    "result = np.column_stack([y_pred,y_test])\n",
    "result = pd.DataFrame(result,columns=['pred','y_test'])\n",
    "pd.crosstab(result.pred,result.y_test,margins=True)\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2, train_size=0.8)\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "importance_value = rf.feature_importances_\n",
    "importance_var = list(x_train.columns)\n",
    "importance_var_value = []\n",
    "for i in range(len(importance_var)):\n",
    "    importance_var_value.append([importance_var[i], importance_value[i]])\n",
    "print(importance_var_value)\n",
    "importance_df = pd.DataFrame()\n",
    "importance_df = importance_df.append(importance_var_value)\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "print (classification_report(y_test,y_pred,target_names=['class_0','class_1'],digits=4)) \n",
    "\n",
    "result = np.column_stack([y_pred,y_test])\n",
    "result = pd.DataFrame(result,columns=['pred','y_test'])\n",
    "pd.crosstab(result.pred,result.y_test,margins=True)\n",
    "\n",
    "\n",
    "# In[183]:\n",
    "\n",
    "\n",
    "y_pred\n",
    "\n",
    "\n",
    "# In[187]:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# In[188]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression()\n",
    "\n",
    "\n",
    "# In[189]:\n",
    "\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# In[191]:\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "print (classification_report(y_test,y_pred,target_names=['class_0','class_1'],digits=4)) \n",
    "\n",
    "result = np.column_stack([y_pred,y_test])\n",
    "result = pd.DataFrame(result,columns=['pred','y_test'])\n",
    "pd.crosstab(result.pred,result.y_test,margins=True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#预测新数据的时候，把预测数据代入，然后把截图的代码跑下\n",
    "\n",
    "\n",
    "# In[231]:\n",
    "\n",
    "\n",
    "y_yuce=rf.predict(x)\n",
    "\n",
    "\n",
    "# In[232]:\n",
    "\n",
    "\n",
    "y_yuce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
